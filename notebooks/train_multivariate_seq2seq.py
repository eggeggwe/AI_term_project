import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_error
import os
import joblib
import matplotlib.pyplot as plt
import matplotlib

# è¨­å®šä¸­æ–‡å­—å‹æ”¯æ´
matplotlib.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False  # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ

# ==========================================
# 1. åƒæ•¸è¨­å®š (Configuration)
# ==========================================
DATA_PATH = "../data/task1_dataset_kotae.csv"
MODEL_SAVE_DIR = "../models"
MODEL_SAVE_PATH = f"{MODEL_SAVE_DIR}/seq2seq_multivariate.pth"
SCALER_SAVE_PATH = f"{MODEL_SAVE_DIR}/scaler_multivariate.pkl"
LOG_SAVE_PATH = f"{MODEL_SAVE_DIR}/eval_log_multivariate.txt"

# æ¨¡å‹è¶…åƒæ•¸
INPUT_SEQ_LEN = 144   # è¼¸å…¥éå» 72 å°æ™‚
OUTPUT_SEQ_LEN = 48   # é æ¸¬æœªä¾† 24 å°æ™‚
BATCH_SIZE = 512
HIDDEN_SIZE = 256
NUM_LAYERS = 4
EPOCHS = 200
PATIENCE = 20  # Early Stopping è€å¿ƒå€¼
LEARNING_RATE = 0.001

# ç‰¹å¾µç¶­åº¦è¨­å®š
INPUT_SIZE = 2        # [äººæ•¸, is_weekend]
OUTPUT_SIZE = 1       # [äººæ•¸]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# ==========================================
# 2. è³‡æ–™è™•ç†èˆ‡æ¨™ç±¤ç”Ÿæˆ (Data Processing)
# ==========================================
def load_and_preprocess_data(path):
    print("Loading raw data...")
    raw_df = pd.read_csv(path)
    
    # --- ä¿®æ­£é»ï¼šå…ˆå°‡åŸå§‹è³‡æ–™èšåˆç®—å‡ºäººæ•¸ ---
    print("Aggregating data to calculate 'number of people'...")
    # è¨ˆç®—æ¯å€‹ (d, t, x, y) çµ„åˆå‡ºç¾äº†å¹¾æ¬¡ (å³äººæ•¸)
    df = raw_df.groupby(['d', 't', 'x', 'y']).size().reset_index(name='number of people')
    
    print(f"Aggregated data shape: {df.shape}")
    
    # --- A. è‡ªå‹•ç”Ÿæˆ Weekend æ¨™ç±¤ (K-Means) ---
    print("Generating 'is_weekend' labels using K-Means...")
    
    # é¸æ“‡ç¸½äººæ•¸æœ€å¤šçš„ç¶²æ ¼ä½œç‚ºåŸºæº–
    # æ³¨æ„ï¼šé€™è£¡è¦ç”¨ groupby sum ä¾†æ‰¾ç¸½äººæ•¸æœ€å¤šçš„é»
    top_grid_idx = df.groupby(['x', 'y'])['number of people'].sum().idxmax()
    print(f"Base grid for clustering: {top_grid_idx}")
    
    base_df = df[(df['x'] == top_grid_idx[0]) & (df['y'] == top_grid_idx[1])].copy()
    
    # æ³¨æ„ï¼šç‚ºäº†èˆ‡ model1.ipynb å°é½Šæ¯”è¼ƒï¼Œæ­¤è™•ç§»é™¤æ™‚é–“è£œé›¶è™•ç†
    # ç›´æ¥ä½¿ç”¨åŸå§‹è³‡æ–™é€²è¡Œå¾ŒçºŒè™•ç†

    # è½‰æˆçŸ©é™£: Index=å¤©æ•¸, Columns=æ™‚é–“é»
    pivot_matrix = base_df.pivot(index='d', columns='t', values='number of people').fillna(0)
    
    # K-Means åˆ†ç¾¤
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(pivot_matrix)
    labels = kmeans.labels_
    
    # åˆ¤æ–·å“ªä¸€ç¾¤æ˜¯é€±æœ«
    c0_idx = np.where(labels == 0)[0]
    c1_idx = np.where(labels == 1)[0]
    
    # æ¯”è¼ƒ t=16 (æ—©ä¸Š8é») çš„å¹³å‡äººæµ
    if len(c0_idx) > 0 and len(c1_idx) > 0:
        avg_flow_0 = pivot_matrix.iloc[c0_idx, 16].mean()
        avg_flow_1 = pivot_matrix.iloc[c1_idx, 16].mean()
        weekend_label_cluster = 0 if avg_flow_0 < avg_flow_1 else 1
    else:
        # æ¥µç«¯æƒ…æ³è™•ç†
        weekend_label_cluster = 1 if len(c0_idx) < len(c1_idx) else 0

    # å»ºç«‹æ˜ å°„è¡¨
    day_is_weekend = [1 if l == weekend_label_cluster else 0 for l in labels]
    label_map = pd.DataFrame({'d': pivot_matrix.index, 'is_weekend': day_is_weekend})
    
    print(f"Weekday count: {len(label_map[label_map['is_weekend']==0])}")
    print(f"Weekend count: {len(label_map[label_map['is_weekend']==1])}")
    
    # --- B. ç¯©é¸å‰ä¸‰å¤§ç†±é»ä¸¦åˆä½µæ¨™ç±¤ ---
    print("Selecting Top 3 locations...")
    # æ‰¾å‡ºç¸½äººæ•¸å‰ä¸‰å¤šçš„åœ°é»
    top_3 = df.groupby(['x', 'y'])['number of people'].sum().nlargest(3).reset_index()[['x', 'y']]
    
    # åªä¿ç•™é€™ä¸‰å€‹åœ°é»çš„è³‡æ–™
    result_df = pd.merge(df, top_3, on=['x', 'y'], how='inner')
    
    # åˆä½µ K-Means ç”¢ç”Ÿçš„æ¨™ç±¤
    result_df = pd.merge(result_df, label_map, on='d', how='left')
    
    # --- C. æ¨™æº–åŒ– (Normalization) ---
    scaler = MinMaxScaler()
    result_df['number_scaled'] = scaler.fit_transform(result_df[['number of people']])
    
    return result_df, scaler

# ==========================================
# 3. è‡ªå®šç¾© Dataset (Multivariate)
# ==========================================
class GridTimeSeriesDataset(Dataset):
    def __init__(self, df, group_by_cols, target_col, aux_col, input_seq_len, output_seq_len):
        self.sequences = []
        
        grouped = df.groupby(group_by_cols)
        
        for _, group_df in grouped:
            # æ­£ç¢ºçš„æ’åºæ–¹å¼ï¼šæŒ‰ ['d', 't'] æ’åº
            group_df = group_df.sort_values(['d', 't'])
            
            target_vals = group_df[target_col].values
            aux_vals = group_df[aux_col].values
            
            total_len = len(target_vals)
            
            # æ»‘å‹•è¦–çª—ç”Ÿæˆåºåˆ—
            for i in range(total_len - input_seq_len - output_seq_len + 1):
                in_target = target_vals[i : i + input_seq_len]
                in_aux = aux_vals[i : i + input_seq_len]
                
                # Stack: (Seq_Len, 2)
                input_seq = np.stack((in_target, in_aux), axis=1)
                
                # Output: (Output_Len)
                output_seq = target_vals[i + input_seq_len : i + input_seq_len + output_seq_len]
                
                self.sequences.append((input_seq, output_seq))
                
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        input_seq, output_seq = self.sequences[idx]
        return torch.FloatTensor(input_seq), torch.FloatTensor(output_seq).unsqueeze(-1)

# ==========================================
# 4. æ¨¡å‹æ¶æ§‹ (Seq2Seq)
# ==========================================
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(Encoder, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

    def forward(self, x):
        _, (hidden, cell) = self.lstm(x)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_size, hidden_size, num_layers):
        super(Decoder, self).__init__()
        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden, cell):
        output, (hidden, cell) = self.lstm(x, (hidden, cell))
        prediction = self.fc(output)
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, target_len, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.target_len = target_len
        self.device = device

    def forward(self, source):
        batch_size = source.shape[0]
        hidden, cell = self.encoder(source)
        
        # Decoder åˆå§‹è¼¸å…¥è¨­ç‚º 0
        decoder_input = torch.zeros(batch_size, 1, 1).to(self.device)
        
        outputs = []
        for _ in range(self.target_len):
            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)
            outputs.append(prediction)
            decoder_input = prediction 
            
        outputs = torch.cat(outputs, dim=1) 
        return outputs

# ==========================================
# 5. ä¸»åŸ·è¡Œæµç¨‹
# ==========================================
if __name__ == "__main__":
    
    # --- Step 1: æº–å‚™è³‡æ–™ ---
    df, scaler = load_and_preprocess_data(DATA_PATH)
    
    print("Creating dataset...")
    dataset = GridTimeSeriesDataset(
        df, 
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        aux_col='is_weekend',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    if len(dataset) == 0:
        print("Error: Dataset is empty. Please check input sequence length and data continuity.")
        exit()

    # --- å›ºå®šåˆ‡åˆ†ï¼š40 å¤©è¨“ç·´ï¼Œ10 å¤©é©—è­‰ï¼Œ25 å¤©æ¸¬è©¦ ---
    TRAIN_DAYS = 40
    VAL_DAYS = 10
    TEST_DAYS = 25
    TOTAL_DAYS = 75
    
    # é‡æ–°å»ºç«‹ Datasetï¼šåˆ†åˆ¥ç‚ºè¨“ç·´é›†ã€é©—è­‰é›†å’Œæ¸¬è©¦é›†
    train_df = df[df['d'] < TRAIN_DAYS]
    val_df = df[(df['d'] >= TRAIN_DAYS) & (df['d'] < TRAIN_DAYS + VAL_DAYS)]
    test_df = df[df['d'] >= TRAIN_DAYS + VAL_DAYS]
    
    print(f"è¨“ç·´é›†å¤©æ•¸: 0 ~ {TRAIN_DAYS-1} (å…± {TRAIN_DAYS} å¤©)")
    print(f"é©—è­‰é›†å¤©æ•¸: {TRAIN_DAYS} ~ {TRAIN_DAYS+VAL_DAYS-1} (å…± {VAL_DAYS} å¤©)")
    print(f"æ¸¬è©¦é›†å¤©æ•¸: {TRAIN_DAYS+VAL_DAYS} ~ {TOTAL_DAYS-1} (å…± {TEST_DAYS} å¤©)")
    
    train_dataset = GridTimeSeriesDataset(
        train_df, 
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        aux_col='is_weekend',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    val_dataset = GridTimeSeriesDataset(
        val_df, 
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        aux_col='is_weekend',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    test_dataset = GridTimeSeriesDataset(
        test_df, 
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        aux_col='is_weekend',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Validation samples: {len(val_dataset)}")
    print(f"Testing samples: {len(test_dataset)}")
    
    # --- Step 2: å»ºç«‹æ¨¡å‹ ---
    encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)
    decoder = Decoder(OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)
    model = Seq2Seq(encoder, decoder, OUTPUT_SEQ_LEN, DEVICE).to(DEVICE)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    
    # --- Step 3: è¨“ç·´è¿´åœˆ (å« Early Stopping) ---
    print("Starting training...")
    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None
    
    for epoch in range(EPOCHS):
        model.train()
        total_train_loss = 0
        
        for x, y in train_loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            
            total_train_loss += loss.item()
            
        avg_train_loss = total_train_loss / len(train_loader)
        
        # é©—è­‰éšæ®µï¼šä½¿ç”¨é©—è­‰é›†è¨ˆç®— validation loss
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(DEVICE), y.to(DEVICE)
                output = model(x)
                val_loss = criterion(output, y)
                total_val_loss += val_loss.item()
        avg_val_loss = total_val_loss / len(val_loader)
        
        print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f}")
        
        # Early Stopping æª¢æŸ¥
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            print(f"  âœ“ Best model updated (Val Loss: {best_val_loss:.6f})")
        else:
            patience_counter += 1
            print(f"  No improvement ({patience_counter}/{PATIENCE})")
            
        if patience_counter >= PATIENCE:
            print(f"\nEarly stopping triggered at epoch {epoch+1}!")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹ä¸¦å„²å­˜
    if best_model_state is not None:
        model.load_state_dict(best_model_state)
    
    torch.save({
        'model_state_dict': model.state_dict(),
        'hyperparameters': {
            'input_size': INPUT_SIZE,
            'hidden_size': HIDDEN_SIZE,
            'num_layers': NUM_LAYERS,
            'input_seq_len': INPUT_SEQ_LEN,
            'output_seq_len': OUTPUT_SEQ_LEN
        }
    }, MODEL_SAVE_PATH)
            
    print(f"\nTraining complete. Best model saved to {MODEL_SAVE_PATH}")
    joblib.dump(scaler, SCALER_SAVE_PATH)
    
    # --- Step 4: è©•ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨æ¸¬è©¦é›†ï¼‰---
    print("\n--- æ¨¡å‹è©•ä¼° ---")
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            output = model(x)
            all_preds.append(output.cpu().numpy())
            all_targets.append(y.numpy())
    
    preds = np.concatenate(all_preds, axis=0).reshape(-1, 1)
    targets = np.concatenate(all_targets, axis=0).reshape(-1, 1)
    
    preds_original = scaler.inverse_transform(preds).flatten()
    targets_original = scaler.inverse_transform(targets).flatten()
    
    mse = mean_squared_error(targets_original, preds_original)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(targets_original, preds_original)
    
    print(f"ğŸ“ˆ æ¨¡å‹è©•ä¼°çµæœ (æ¸¬è©¦é›†):")
    print(f"   MSE: {mse:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   MAE: {mae:.4f}")
    
    # å„²å­˜è©•ä¼°çµæœåˆ° log æª”
    with open(LOG_SAVE_PATH, 'w', encoding='utf-8') as f:
        f.write("=" * 50 + "\n")
        f.write("æ¨¡å‹: Multivariate Seq2Seq (äººæ•¸ + is_weekend)\n")
        f.write("=" * 50 + "\n")
        f.write(f"è¼¸å…¥ç‰¹å¾µæ•¸: {INPUT_SIZE}\n")
        f.write(f"è¨“ç·´é›†: å‰ 50 å¤© (æ¨£æœ¬æ•¸: {len(train_dataset)})\n")
        f.write(f"æ¸¬è©¦é›†: å¾Œ 25 å¤© (æ¨£æœ¬æ•¸: {len(test_dataset)})\n")
        f.write("\n--- æ¸¬è©¦é›†è©•ä¼°çµæœ ---\n")
        f.write(f"MSE:  {mse:.4f}\n")
        f.write(f"RMSE: {rmse:.4f}\n")
        f.write(f"MAE:  {mae:.4f}\n")
        f.write("=" * 50 + "\n")
    print(f"ğŸ“ è©•ä¼°çµæœå·²å„²å­˜è‡³ {LOG_SAVE_PATH}")
    model.eval()
    
    # æ™ºæ…§æœå°‹ï¼šåœ¨æ¸¬è©¦é›†ä¸­æœå°‹äººæµæ³¢å‹•æ˜é¡¯çš„æ¨£æœ¬
    total_len = len(test_dataset)
    print(f"æ¸¬è©¦é›†ç¸½æ•¸: {total_len}")
    
    # æœå°‹ç†±é–€æ™‚æ®µ (High-traffic sample)
    target_sample_idx = 0
    found = False
    
    print("æ­£åœ¨æœå°‹äººæµæ³¢å‹•æ˜é¡¯çš„æ¨£æœ¬ (Max > 80)...")
    
    for i in range(total_len):
        _, target_tensor = test_dataset[i]
        # åªå–äººæ•¸éƒ¨åˆ†ï¼ˆç¬¬ä¸€å€‹ç‰¹å¾µï¼‰é€²è¡Œåæ¨™æº–åŒ–
        temp_val = scaler.inverse_transform(target_tensor.numpy().reshape(-1, 1))
        
        if temp_val.max() > 80:
            target_sample_idx = i
            print(f"\u2705 æ‰¾åˆ°ç›®æ¨™æ¨£æœ¬ Index: {i} (æœ€å¤§äººæµ: {temp_val.max():.2f})")
            found = True
            break
    
    if not found:
        print("\u26a0\ufe0f æœªæ‰¾åˆ° > 80 çš„æ¨£æœ¬ï¼Œå°‡ä½¿ç”¨æ¸¬è©¦é›†çš„ç¬¬ä¸€ç­†è³‡æ–™ç¹ªåœ–ã€‚")
    
    # å–å¾—æ¨£æœ¬é€²è¡Œé æ¸¬
    sample_input, sample_target = test_dataset[target_sample_idx]
    sample_input = sample_input.unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        prediction = model(sample_input).cpu().numpy().reshape(-1, 1)
        target = sample_target.numpy().reshape(-1, 1)
        
    pred_orig = scaler.inverse_transform(prediction)
    target_orig = scaler.inverse_transform(target)
    
    plt.figure(figsize=(10, 5))
    plt.plot(target_orig, label='Actual (Ground Truth)', linewidth=2)
    plt.plot(pred_orig, label='Predicted (Multivariate Model)', linestyle='--', color='orange', linewidth=2)
    plt.title(f"Multivariate Seq2Seq Prediction (Sample Index: {target_sample_idx})")
    plt.xlabel("Time Steps (Next 24 Hours)")
    plt.ylabel("Number of People")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f"{MODEL_SAVE_DIR}/prediction_result_multivariate.png")
    plt.show()
    print(f"Result plot saved to {MODEL_SAVE_DIR}/prediction_result_multivariate.png")
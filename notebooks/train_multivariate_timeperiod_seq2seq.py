import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_error
import os
import joblib
import matplotlib.pyplot as plt

# ==========================================
# 1. åƒæ•¸è¨­å®š (Configuration)
# ==========================================
DATA_PATH = "../data/task1_dataset_kotae.csv"
MODEL_SAVE_DIR = "../models"
MODEL_SAVE_PATH = f"{MODEL_SAVE_DIR}/seq2seq_multivariate_timeperiod.pth"
SCALER_SAVE_PATH = f"{MODEL_SAVE_DIR}/scaler_multivariate_timeperiod.pkl"
LOG_SAVE_PATH = f"{MODEL_SAVE_DIR}/eval_log_multivariate_timeperiod.txt"

# æ¨¡å‹è¶…åƒæ•¸ï¼ˆèˆ‡ model1, multivariate ç›¸åŒï¼‰
INPUT_SEQ_LEN = 144   # è¼¸å…¥éå» 72 å°æ™‚
OUTPUT_SEQ_LEN = 48   # é æ¸¬æœªä¾† 24 å°æ™‚
BATCH_SIZE = 512
HIDDEN_SIZE = 256
NUM_LAYERS = 4
EPOCHS = 20
LEARNING_RATE = 0.001

# ç‰¹å¾µç¶­åº¦è¨­å®š
# [äººæ•¸, is_weekend, time_period_0, time_period_1, time_period_2, time_period_3]
# æ™‚æ®µä½¿ç”¨ One-Hot ç·¨ç¢¼ (4ç¶­)ï¼Œæ‰€ä»¥ç¸½å…± 1 + 1 + 4 = 6 ç¶­
INPUT_SIZE = 6        
OUTPUT_SIZE = 1       # [äººæ•¸]

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# ==========================================
# 2. è³‡æ–™è™•ç†èˆ‡æ¨™ç±¤ç”Ÿæˆ (Data Processing)
# ==========================================
def get_time_period(t):
    """
    æ ¹æ“šæ™‚é–“é» t (0-47) åˆ†é¡ç‚ºä¸åŒæ™‚æ®µ
    æ¯å€‹ t ä»£è¡¨ 30 åˆ†é˜ï¼Œæ‰€ä»¥ï¼š
    - t=0 ä»£è¡¨ 00:00, t=1 ä»£è¡¨ 00:30, ...
    - t=16 ä»£è¡¨ 08:00, t=24 ä»£è¡¨ 12:00, ...
    
    æ™‚æ®µåˆ†é¡ï¼š
    - 0: æ·±å¤œ (00:00 - 06:00) -> t: 0-11
    - 1: æ—©ä¸Š (06:00 - 12:00) -> t: 12-23
    - 2: ä¸‹åˆ (12:00 - 18:00) -> t: 24-35
    - 3: æ™šä¸Š (18:00 - 24:00) -> t: 36-47
    """
    if t < 12:
        return 0  # æ·±å¤œ
    elif t < 24:
        return 1  # æ—©ä¸Š
    elif t < 36:
        return 2  # ä¸‹åˆ
    else:
        return 3  # æ™šä¸Š

def load_and_preprocess_data(path):
    print("Loading raw data...")
    raw_df = pd.read_csv(path)
    
    # --- ä¿®æ­£é»ï¼šå…ˆå°‡åŸå§‹è³‡æ–™èšåˆç®—å‡ºäººæ•¸ ---
    print("Aggregating data to calculate 'number of people'...")
    df = raw_df.groupby(['d', 't', 'x', 'y']).size().reset_index(name='number of people')
    
    print(f"Aggregated data shape: {df.shape}")
    
    # --- A. è‡ªå‹•ç”Ÿæˆ Weekend æ¨™ç±¤ (K-Means) ---
    print("Generating 'is_weekend' labels using K-Means...")
    
    # é¸æ“‡ç¸½äººæ•¸æœ€å¤šçš„ç¶²æ ¼ä½œç‚ºåŸºæº–
    top_grid_idx = df.groupby(['x', 'y'])['number of people'].sum().idxmax()
    print(f"Base grid for clustering: {top_grid_idx}")
    
    base_df = df[(df['x'] == top_grid_idx[0]) & (df['y'] == top_grid_idx[1])].copy()
    
    # æ³¨æ„ï¼šç‚ºäº†èˆ‡ model1.ipynb å°é½Šæ¯”è¼ƒï¼Œæ­¤è™•ç§»é™¤æ™‚é–“è£œé›¶è™•ç†
    # ç›´æ¥ä½¿ç”¨åŸå§‹è³‡æ–™é€²è¡Œå¾ŒçºŒè™•ç†

    # è½‰æˆçŸ©é™£: Index=å¤©æ•¸, Columns=æ™‚é–“é»
    pivot_matrix = base_df.pivot(index='d', columns='t', values='number of people').fillna(0)
    
    # K-Means åˆ†ç¾¤
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(pivot_matrix)
    labels = kmeans.labels_
    
    # åˆ¤æ–·å“ªä¸€ç¾¤æ˜¯é€±æœ«
    c0_idx = np.where(labels == 0)[0]
    c1_idx = np.where(labels == 1)[0]
    
    # æ¯”è¼ƒ t=16 (æ—©ä¸Š8é») çš„å¹³å‡äººæµ
    if len(c0_idx) > 0 and len(c1_idx) > 0:
        avg_flow_0 = pivot_matrix.iloc[c0_idx, 16].mean()
        avg_flow_1 = pivot_matrix.iloc[c1_idx, 16].mean()
        weekend_label_cluster = 0 if avg_flow_0 < avg_flow_1 else 1
    else:
        # æ¥µç«¯æƒ…æ³è™•ç†
        weekend_label_cluster = 1 if len(c0_idx) < len(c1_idx) else 0

    # å»ºç«‹æ˜ å°„è¡¨
    day_is_weekend = [1 if l == weekend_label_cluster else 0 for l in labels]
    label_map = pd.DataFrame({'d': pivot_matrix.index, 'is_weekend': day_is_weekend})
    
    print(f"Weekday count: {len(label_map[label_map['is_weekend']==0])}")
    print(f"Weekend count: {len(label_map[label_map['is_weekend']==1])}")
    
    # --- B. ç”Ÿæˆæ™‚æ®µæ¨™ç±¤ (One-Hot ç·¨ç¢¼) ---
    print("Generating 'time_period' labels with One-Hot encoding...")
    df['time_period'] = df['t'].apply(get_time_period)
    
    # One-Hot ç·¨ç¢¼ï¼šå»ºç«‹ 4 å€‹æ¬„ä½
    df['time_period_0'] = (df['time_period'] == 0).astype(float)  # æ·±å¤œ
    df['time_period_1'] = (df['time_period'] == 1).astype(float)  # æ—©ä¸Š
    df['time_period_2'] = (df['time_period'] == 2).astype(float)  # ä¸‹åˆ
    df['time_period_3'] = (df['time_period'] == 3).astype(float)  # æ™šä¸Š
    
    period_counts = df['time_period'].value_counts().sort_index()
    print(f"æ™‚æ®µåˆ†å¸ƒ: æ·±å¤œ(0)={period_counts.get(0, 0)}, æ—©ä¸Š(1)={period_counts.get(1, 0)}, ä¸‹åˆ(2)={period_counts.get(2, 0)}, æ™šä¸Š(3)={period_counts.get(3, 0)}")
    print("ä½¿ç”¨ One-Hot ç·¨ç¢¼è¡¨ç¤ºæ™‚æ®µç‰¹å¾µ (4ç¶­)")
    
    # --- C. ç¯©é¸å‰ä¸‰å¤§ç†±é»ä¸¦åˆä½µæ¨™ç±¤ ---
    print("Selecting Top 3 locations...")
    top_3 = df.groupby(['x', 'y'])['number of people'].sum().nlargest(3).reset_index()[['x', 'y']]
    
    # åªä¿ç•™é€™ä¸‰å€‹åœ°é»çš„è³‡æ–™
    result_df = pd.merge(df, top_3, on=['x', 'y'], how='inner')
    
    # åˆä½µ K-Means ç”¢ç”Ÿçš„æ¨™ç±¤
    result_df = pd.merge(result_df, label_map, on='d', how='left')
    
    # --- D. æ¨™æº–åŒ– (Normalization) ---
    scaler = MinMaxScaler()
    result_df['number_scaled'] = scaler.fit_transform(result_df[['number of people']])
    
    return result_df, scaler

# ==========================================
# 3. è‡ªå®šç¾© Dataset (Multivariate with One-Hot Time Period)
# ==========================================
class GridTimeSeriesDataset(Dataset):
    def __init__(self, df, group_by_cols, target_col, input_seq_len, output_seq_len):
        """
        ä½¿ç”¨ One-Hot ç·¨ç¢¼çš„æ™‚æ®µç‰¹å¾µ
        è¼¸å…¥ç‰¹å¾µ: [äººæ•¸, is_weekend, time_period_0, time_period_1, time_period_2, time_period_3]
        """
        self.sequences = []
        
        grouped = df.groupby(group_by_cols)
        
        for _, group_df in grouped:
            # æ­£ç¢ºçš„æ’åºæ–¹å¼ï¼šæŒ‰ ['d', 't'] æ’åº
            group_df = group_df.sort_values(['d', 't'])
            
            target_vals = group_df[target_col].values
            is_weekend_vals = group_df['is_weekend'].values
            tp0_vals = group_df['time_period_0'].values
            tp1_vals = group_df['time_period_1'].values
            tp2_vals = group_df['time_period_2'].values
            tp3_vals = group_df['time_period_3'].values
            
            total_len = len(target_vals)
            
            # æ»‘å‹•è¦–çª—ç”Ÿæˆåºåˆ—
            for i in range(total_len - input_seq_len - output_seq_len + 1):
                in_target = target_vals[i : i + input_seq_len]
                in_weekend = is_weekend_vals[i : i + input_seq_len]
                in_tp0 = tp0_vals[i : i + input_seq_len]
                in_tp1 = tp1_vals[i : i + input_seq_len]
                in_tp2 = tp2_vals[i : i + input_seq_len]
                in_tp3 = tp3_vals[i : i + input_seq_len]
                
                # Stack: (Seq_Len, 6) - äººæ•¸ + is_weekend + 4å€‹ One-Hot æ™‚æ®µ
                input_seq = np.stack((in_target, in_weekend, in_tp0, in_tp1, in_tp2, in_tp3), axis=1)
                
                # Output: (Output_Len)
                output_seq = target_vals[i + input_seq_len : i + input_seq_len + output_seq_len]
                
                self.sequences.append((input_seq, output_seq))
                
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        input_seq, output_seq = self.sequences[idx]
        return torch.FloatTensor(input_seq), torch.FloatTensor(output_seq).unsqueeze(-1)

# ==========================================
# 4. æ¨¡å‹æ¶æ§‹ (Seq2Seq)
# ==========================================
class Encoder(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(Encoder, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)

    def forward(self, x):
        _, (hidden, cell) = self.lstm(x)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_size, hidden_size, num_layers):
        super(Decoder, self).__init__()
        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x, hidden, cell):
        output, (hidden, cell) = self.lstm(x, (hidden, cell))
        prediction = self.fc(output)
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, target_len, device):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.target_len = target_len
        self.device = device

    def forward(self, source):
        batch_size = source.shape[0]
        hidden, cell = self.encoder(source)
        
        # Decoder åˆå§‹è¼¸å…¥è¨­ç‚º 0
        decoder_input = torch.zeros(batch_size, 1, 1).to(self.device)
        
        outputs = []
        for _ in range(self.target_len):
            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)
            outputs.append(prediction)
            decoder_input = prediction 
            
        outputs = torch.cat(outputs, dim=1) 
        return outputs

# ==========================================
# 5. ä¸»åŸ·è¡Œæµç¨‹
# ==========================================
if __name__ == "__main__":
    
    # --- Step 1: æº–å‚™è³‡æ–™ ---
    df, scaler = load_and_preprocess_data(DATA_PATH)
    
    # --- å›ºå®šåˆ‡åˆ†ï¼šå‰ 50 å¤©è¨“ç·´ï¼Œå¾Œ 25 å¤©æ¸¬è©¦ ---
    TRAIN_DAYS = 50
    TEST_DAYS = 25
    TOTAL_DAYS = 75
    
    # é‡æ–°å»ºç«‹ Datasetï¼šåˆ†åˆ¥ç‚ºè¨“ç·´é›†å’Œæ¸¬è©¦é›†
    train_df = df[df['d'] < TRAIN_DAYS]
    test_df = df[df['d'] >= TRAIN_DAYS]
    
    print(f"è¨“ç·´é›†å¤©æ•¸: 0 ~ {TRAIN_DAYS-1} (å…± {TRAIN_DAYS} å¤©)")
    print(f"æ¸¬è©¦é›†å¤©æ•¸: {TRAIN_DAYS} ~ {TOTAL_DAYS-1} (å…± {TEST_DAYS} å¤©)")
    
    print("Creating dataset with One-Hot time period features...")
    train_dataset = GridTimeSeriesDataset(
        train_df, 
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    test_dataset = GridTimeSeriesDataset(
        test_df, 
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    if len(train_dataset) == 0:
        print("Error: Train dataset is empty. Please check input sequence length and data continuity.")
        exit()
    
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
    
    print(f"Training samples: {len(train_dataset)}")
    print(f"Testing samples: {len(test_dataset)}")
    
    # --- Step 2: å»ºç«‹æ¨¡å‹ ---
    encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)
    decoder = Decoder(OUTPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS).to(DEVICE)
    model = Seq2Seq(encoder, decoder, OUTPUT_SEQ_LEN, DEVICE).to(DEVICE)
    
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    criterion = nn.MSELoss()
    
    # --- Step 3: è¨“ç·´è¿´åœˆ ---
    print("Starting training...")
    best_val_loss = float('inf')
    
    for epoch in range(EPOCHS):
        model.train()
        total_train_loss = 0
        
        for x, y in train_loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            
            optimizer.zero_grad()
            output = model(x)
            loss = criterion(output, y)
            loss.backward()
            optimizer.step()
            
            total_train_loss += loss.item()
            
        avg_train_loss = total_train_loss / len(train_loader)
        
        print(f"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.6f}")
        
        # æ¯å€‹ epoch éƒ½å„²å­˜æ¨¡å‹ï¼ˆèˆ‡ model1 é¡ä¼¼ï¼Œä¸åš early stoppingï¼‰
    
    # è¨“ç·´çµæŸå¾Œå„²å­˜æ¨¡å‹
    torch.save({
        'model_state_dict': model.state_dict(),
        'hyperparameters': {
            'input_size': INPUT_SIZE,
            'hidden_size': HIDDEN_SIZE,
            'num_layers': NUM_LAYERS,
            'input_seq_len': INPUT_SEQ_LEN,
            'output_seq_len': OUTPUT_SEQ_LEN
        }
    }, MODEL_SAVE_PATH)
            
    print(f"Training complete. Model saved to {MODEL_SAVE_PATH}")
    joblib.dump(scaler, SCALER_SAVE_PATH)
    
    # --- Step 4: è©•ä¼°æ¨¡å‹ï¼ˆä½¿ç”¨æ¸¬è©¦é›†ï¼‰---
    print("\n--- æ¨¡å‹è©•ä¼° ---")
    model.eval()
    all_preds = []
    all_targets = []
    
    with torch.no_grad():
        for x, y in test_loader:
            x = x.to(DEVICE)
            output = model(x)
            all_preds.append(output.cpu().numpy())
            all_targets.append(y.numpy())
    
    preds = np.concatenate(all_preds, axis=0).reshape(-1, 1)
    targets = np.concatenate(all_targets, axis=0).reshape(-1, 1)
    
    preds_original = scaler.inverse_transform(preds).flatten()
    targets_original = scaler.inverse_transform(targets).flatten()
    
    mse = mean_squared_error(targets_original, preds_original)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(targets_original, preds_original)
    
    print(f"ğŸ“ˆ æ¨¡å‹è©•ä¼°çµæœ (æ¸¬è©¦é›†):")
    print(f"   MSE: {mse:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   MAE: {mae:.4f}")
    
    # å„²å­˜è©•ä¼°çµæœåˆ° log æª”
    with open(LOG_SAVE_PATH, 'w', encoding='utf-8') as f:
        f.write("=" * 50 + "\n")
        f.write("æ¨¡å‹: Multivariate + Time Period Seq2Seq (One-Hot)\n")
        f.write("=" * 50 + "\n")
        f.write(f"è¼¸å…¥ç‰¹å¾µ: [äººæ•¸, is_weekend, æ™‚æ®µ One-Hot (4ç¶­)]\n")
        f.write(f"è¼¸å…¥ç‰¹å¾µæ•¸: {INPUT_SIZE}\n")
        f.write(f"è¨“ç·´é›†: å‰ 50 å¤© (æ¨£æœ¬æ•¸: {len(train_dataset)})\n")
        f.write(f"æ¸¬è©¦é›†: å¾Œ 25 å¤© (æ¨£æœ¬æ•¸: {len(test_dataset)})\n")
        f.write("\n--- æ¸¬è©¦é›†è©•ä¼°çµæœ ---\n")
        f.write(f"MSE:  {mse:.4f}\n")
        f.write(f"RMSE: {rmse:.4f}\n")
        f.write(f"MAE:  {mae:.4f}\n")
        f.write("=" * 50 + "\n")
    print(f"ğŸ“ è©•ä¼°çµæœå·²å„²å­˜è‡³ {LOG_SAVE_PATH}")
    
    # --- Step 5: ç•«åœ–ï¼ˆä½¿ç”¨æ¸¬è©¦é›†ï¼‰---
    model.eval()
    
    # æ™ºæ…§æœå°‹ï¼šåœ¨æ¸¬è©¦é›†ä¸­æœå°‹äººæµæ³¢å‹•æ˜é¡¯çš„æ¨£æœ¬
    total_len = len(test_dataset)
    print(f"æ¸¬è©¦é›†ç¸½æ•¸: {total_len}")
    
    # æœå°‹ç†±é–€æ™‚æ®µ (High-traffic sample)
    target_sample_idx = 0
    found = False
    
    print("æ­£åœ¨æœå°‹äººæµæ³¢å‹•æ˜é¡¯çš„æ¨£æœ¬ (Max > 80)...")
    
    for i in range(total_len):
        _, target_tensor = test_dataset[i]
        # åªå–äººæ•¸éƒ¨åˆ†ï¼ˆç¬¬ä¸€å€‹ç‰¹å¾µï¼‰é€²è¡Œåæ¨™æº–åŒ–
        temp_val = scaler.inverse_transform(target_tensor.numpy().reshape(-1, 1))
        
        if temp_val.max() > 80:
            target_sample_idx = i
            print(f"âœ… æ‰¾åˆ°ç›®æ¨™æ¨£æœ¬ Index: {i} (æœ€å¤§äººæµ: {temp_val.max():.2f})")
            found = True
            break
    
    if not found:
        print("âš ï¸ æœªæ‰¾åˆ° > 80 çš„æ¨£æœ¬ï¼Œå°‡ä½¿ç”¨æ¸¬è©¦é›†çš„ç¬¬ä¸€ç­†è³‡æ–™ç¹ªåœ–ã€‚")
    
    # å–å¾—æ¨£æœ¬é€²è¡Œé æ¸¬
    sample_input, sample_target = test_dataset[target_sample_idx]
    sample_input = sample_input.unsqueeze(0).to(DEVICE)
    
    with torch.no_grad():
        prediction = model(sample_input).cpu().numpy().reshape(-1, 1)
        target = sample_target.numpy().reshape(-1, 1)
        
    pred_orig = scaler.inverse_transform(prediction)
    target_orig = scaler.inverse_transform(target)
    
    plt.figure(figsize=(10, 5))
    plt.plot(target_orig, label='Actual (Ground Truth)', linewidth=2)
    plt.plot(pred_orig, label='Predicted (Multivariate + Time Period One-Hot)', linestyle='--', color='orange', linewidth=2)
    plt.title(f"Multivariate + Time Period (One-Hot) Seq2Seq Prediction (Sample Index: {target_sample_idx})")
    plt.xlabel("Time Steps (Next 24 Hours)")
    plt.ylabel("Number of People")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f"{MODEL_SAVE_DIR}/prediction_result_multivariate_timeperiod.png")
    plt.show()
    print(f"Result plot saved to {MODEL_SAVE_DIR}/prediction_result_multivariate_timeperiod.png")

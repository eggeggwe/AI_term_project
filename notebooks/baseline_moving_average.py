"""
AI æœŸæœ«ä½œæ¥­ - Moving Average Baseline æ¨¡å‹
==========================================
æ­¤è…³æœ¬å¯¦ä½œç§»å‹•å¹³å‡åŸºæº–æ¨¡å‹ï¼Œä½œç‚º Seq2Seq æ¨¡å‹çš„æ¯”è¼ƒå°è±¡ã€‚
ä½¿ç”¨èˆ‡å…¶ä»–æ¨¡å‹å®Œå…¨ç›¸åŒçš„è³‡æ–™åˆ‡åˆ†é‚è¼¯ (40/10/25 å¤©) å’Œè©•ä¼°æŒ‡æ¨™ã€‚
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import matplotlib

# è¨­å®šä¸­æ–‡å­—å‹æ”¯æ´
matplotlib.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False  # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ

import os

# ==========================================
# 1. åƒæ•¸è¨­å®š (èˆ‡å…¶ä»–æ¨¡å‹ä¸€è‡´)
# ==========================================
DATA_PATH = "../data/task1_dataset_kotae.csv"
MODEL_SAVE_DIR = "../models"
LOG_SAVE_PATH = f"{MODEL_SAVE_DIR}/eval_log_baseline_ma.txt"

# åºåˆ—é•·åº¦è¨­å®š (èˆ‡ Seq2Seq æ¨¡å‹ä¸€è‡´)
INPUT_SEQ_LEN = 144   # è¼¸å…¥éå» 72 å°æ™‚ (ç”¨æ–¼è¨ˆç®—ç§»å‹•å¹³å‡)
OUTPUT_SEQ_LEN = 48   # é æ¸¬æœªä¾† 24 å°æ™‚

# è³‡æ–™åˆ‡åˆ†è¨­å®š (èˆ‡å…¶ä»–æ¨¡å‹ä¸€è‡´)
TRAIN_DAYS = 40
VAL_DAYS = 10
TEST_DAYS = 25
TOTAL_DAYS = 75

os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

# ==========================================
# 2. è³‡æ–™è™•ç† (èˆ‡å…¶ä»–æ¨¡å‹ç›¸åŒé‚è¼¯)
# ==========================================
def load_and_preprocess_data(path):
    print("Loading raw data...")
    raw_df = pd.read_csv(path)
    
    # èšåˆè¨ˆç®—äººæ•¸
    print("Aggregating data to calculate 'number of people'...")
    df = raw_df.groupby(['d', 't', 'x', 'y']).size().reset_index(name='number of people')
    print(f"Aggregated data shape: {df.shape}")
    
    # --- A. è‡ªå‹•ç”Ÿæˆ Weekend æ¨™ç±¤ (K-Means) ---
    print("Generating 'is_weekend' labels using K-Means...")
    top_grid_idx = df.groupby(['x', 'y'])['number of people'].sum().idxmax()
    print(f"Base grid for clustering: {top_grid_idx}")
    
    base_df = df[(df['x'] == top_grid_idx[0]) & (df['y'] == top_grid_idx[1])].copy()
    pivot_matrix = base_df.pivot(index='d', columns='t', values='number of people').fillna(0)
    
    kmeans = KMeans(n_clusters=2, random_state=42, n_init=10).fit(pivot_matrix)
    labels = kmeans.labels_
    
    c0_idx = np.where(labels == 0)[0]
    c1_idx = np.where(labels == 1)[0]
    
    if len(c0_idx) > 0 and len(c1_idx) > 0:
        avg_flow_0 = pivot_matrix.iloc[c0_idx, 16].mean()
        avg_flow_1 = pivot_matrix.iloc[c1_idx, 16].mean()
        weekend_label_cluster = 0 if avg_flow_0 < avg_flow_1 else 1
    else:
        weekend_label_cluster = 1 if len(c0_idx) < len(c1_idx) else 0

    day_is_weekend = [1 if l == weekend_label_cluster else 0 for l in labels]
    label_map = pd.DataFrame({'d': pivot_matrix.index, 'is_weekend': day_is_weekend})
    
    print(f"Weekday count: {len(label_map[label_map['is_weekend']==0])}")
    print(f"Weekend count: {len(label_map[label_map['is_weekend']==1])}")
    
    # --- B. ç¯©é¸å‰ä¸‰å¤§ç†±é» ---
    print("Selecting Top 3 locations...")
    top_3 = df.groupby(['x', 'y'])['number of people'].sum().nlargest(3).reset_index()[['x', 'y']]
    result_df = pd.merge(df, top_3, on=['x', 'y'], how='inner')
    result_df = pd.merge(result_df, label_map, on='d', how='left')
    
    # --- C. æ¨™æº–åŒ– ---
    scaler = MinMaxScaler()
    result_df['number_scaled'] = scaler.fit_transform(result_df[['number of people']])
    
    return result_df, scaler

# ==========================================
# 3. Moving Average é æ¸¬å‡½æ•¸
# ==========================================
def moving_average_predict(input_sequence, output_len, window_size=48):
    """
    ä½¿ç”¨ç§»å‹•å¹³å‡ä¾†é æ¸¬æœªä¾†å€¼
    
    Args:
        input_sequence: è¼¸å…¥åºåˆ— (shape: [seq_len])
        output_len: è¦é æ¸¬çš„æ™‚é–“æ­¥æ•¸
        window_size: ç§»å‹•å¹³å‡çª—å£å¤§å° (é è¨­48 = 24å°æ™‚)
    
    Returns:
        predictions: é æ¸¬åºåˆ— (shape: [output_len])
    """
    predictions = []
    
    # è¤‡è£½è¼¸å…¥åºåˆ—ä½œç‚ºå·¥ä½œåºåˆ—
    working_seq = list(input_sequence)
    
    for _ in range(output_len):
        # å–æœ€å¾Œ window_size å€‹å€¼çš„å¹³å‡
        if len(working_seq) >= window_size:
            pred = np.mean(working_seq[-window_size:])
        else:
            pred = np.mean(working_seq)
        
        predictions.append(pred)
        working_seq.append(pred)  # å°‡é æ¸¬å€¼åŠ å…¥åºåˆ—
    
    return np.array(predictions)

# ==========================================
# 4. å»ºç«‹æ¸¬è©¦åºåˆ— (èˆ‡å…¶ä»–æ¨¡å‹ç›¸åŒé‚è¼¯)
# ==========================================
def create_test_sequences(df, group_by_cols, target_col, input_seq_len, output_seq_len):
    """
    å»ºç«‹æ¸¬è©¦åºåˆ—ï¼Œèˆ‡ GridTimeSeriesDataset é‚è¼¯å®Œå…¨ä¸€è‡´
    """
    sequences = []
    
    grouped = df.groupby(group_by_cols)
    
    for _, group_df in grouped:
        group_df = group_df.sort_values(['d', 't'])
        target_vals = group_df[target_col].values
        
        total_len = len(target_vals)
        
        for i in range(total_len - input_seq_len - output_seq_len + 1):
            input_seq = target_vals[i : i + input_seq_len]
            output_seq = target_vals[i + input_seq_len : i + input_seq_len + output_seq_len]
            
            sequences.append((input_seq, output_seq))
    
    return sequences

# ==========================================
# 5. ä¸»åŸ·è¡Œæµç¨‹
# ==========================================
if __name__ == "__main__":
    
    # --- Step 1: æº–å‚™è³‡æ–™ ---
    df, scaler = load_and_preprocess_data(DATA_PATH)
    
    # --- Step 2: åˆ‡åˆ†è³‡æ–™ (èˆ‡å…¶ä»–æ¨¡å‹ä¸€è‡´) ---
    test_df = df[df['d'] >= TRAIN_DAYS + VAL_DAYS]  # Day 50~74
    
    print(f"\n--- è³‡æ–™åˆ‡åˆ† ---")
    print(f"æ¸¬è©¦é›†å¤©æ•¸: {TRAIN_DAYS+VAL_DAYS} ~ {TOTAL_DAYS-1} (å…± {TEST_DAYS} å¤©)")
    
    # --- Step 3: å»ºç«‹æ¸¬è©¦åºåˆ— ---
    test_sequences = create_test_sequences(
        test_df,
        group_by_cols=['x', 'y'],
        target_col='number_scaled',
        input_seq_len=INPUT_SEQ_LEN,
        output_seq_len=OUTPUT_SEQ_LEN
    )
    
    print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {len(test_sequences)}")
    
    if len(test_sequences) == 0:
        print("Error: æ¸¬è©¦é›†ç‚ºç©ºï¼")
        exit()
    
    # --- Step 4: Moving Average é æ¸¬ ---
    print("\n--- Moving Average é æ¸¬ ---")
    
    all_preds = []
    all_targets = []
    
    for input_seq, target_seq in test_sequences:
        pred = moving_average_predict(input_seq, OUTPUT_SEQ_LEN, window_size=48)
        all_preds.append(pred)
        all_targets.append(target_seq)
    
    # åˆä½µæ‰€æœ‰é æ¸¬
    preds = np.concatenate(all_preds).reshape(-1, 1)
    targets = np.concatenate(all_targets).reshape(-1, 1)
    
    # åæ¨™æº–åŒ–
    preds_original = scaler.inverse_transform(preds).flatten()
    targets_original = scaler.inverse_transform(targets).flatten()
    
    # --- Step 5: è¨ˆç®—è©•ä¼°æŒ‡æ¨™ ---
    mse = mean_squared_error(targets_original, preds_original)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(targets_original, preds_original)
    
    print(f"\nğŸ“ˆ Moving Average Baseline è©•ä¼°çµæœ (æ¸¬è©¦é›†):")
    print(f"   MSE: {mse:.4f}")
    print(f"   RMSE: {rmse:.4f}")
    print(f"   MAE: {mae:.4f}")
    
    # --- Step 6: å„²å­˜è©•ä¼°çµæœåˆ° log æª” ---
    with open(LOG_SAVE_PATH, 'w', encoding='utf-8') as f:
        f.write("=" * 50 + "\n")
        f.write("æ¨¡å‹: Moving Average Baseline\n")
        f.write("=" * 50 + "\n")
        f.write(f"çª—å£å¤§å°: 48 (24å°æ™‚)\n")
        f.write(f"è¼¸å…¥åºåˆ—é•·åº¦: {INPUT_SEQ_LEN}\n")
        f.write(f"é æ¸¬åºåˆ—é•·åº¦: {OUTPUT_SEQ_LEN}\n")
        f.write(f"æ¸¬è©¦é›†: å¾Œ 25 å¤© (æ¨£æœ¬æ•¸: {len(test_sequences)})\n")
        f.write("\n--- æ¸¬è©¦é›†è©•ä¼°çµæœ ---\n")
        f.write(f"MSE:  {mse:.4f}\n")
        f.write(f"RMSE: {rmse:.4f}\n")
        f.write(f"MAE:  {mae:.4f}\n")
        f.write("=" * 50 + "\n")
    print(f"ğŸ“ è©•ä¼°çµæœå·²å„²å­˜è‡³ {LOG_SAVE_PATH}")
    
    # --- Step 7: è¦–è¦ºåŒ– (èˆ‡å…¶ä»–æ¨¡å‹ç›¸åŒé‚è¼¯) ---
    # æ™ºæ…§æœå°‹ï¼šåœ¨æ¸¬è©¦é›†ä¸­æœå°‹äººæµæ³¢å‹•æ˜é¡¯çš„æ¨£æœ¬
    total_len = len(test_sequences)
    print(f"\næ¸¬è©¦é›†ç¸½æ•¸: {total_len}")
    
    target_sample_idx = 0
    found = False
    
    print("æ­£åœ¨æœå°‹äººæµæ³¢å‹•æ˜é¡¯çš„æ¨£æœ¬ (Max > 80)...")
    
    for i in range(total_len):
        _, target_seq = test_sequences[i]
        temp_val = scaler.inverse_transform(target_seq.reshape(-1, 1))
        
        if temp_val.max() > 80:
            target_sample_idx = i
            print(f"âœ… æ‰¾åˆ°ç›®æ¨™æ¨£æœ¬ Index: {i} (æœ€å¤§äººæµ: {temp_val.max():.2f})")
            found = True
            break
    
    if not found:
        print("âš ï¸ æœªæ‰¾åˆ° > 80 çš„æ¨£æœ¬ï¼Œå°‡ä½¿ç”¨æ¸¬è©¦é›†çš„ç¬¬ä¸€ç­†è³‡æ–™ç¹ªåœ–ã€‚")
    
    # å–å¾—æ¨£æœ¬é€²è¡Œé æ¸¬
    sample_input, sample_target = test_sequences[target_sample_idx]
    sample_pred = moving_average_predict(sample_input, OUTPUT_SEQ_LEN, window_size=48)
    
    # åæ¨™æº–åŒ–
    pred_orig = scaler.inverse_transform(sample_pred.reshape(-1, 1))
    target_orig = scaler.inverse_transform(sample_target.reshape(-1, 1))
    
    # ç¹ªåœ–
    plt.figure(figsize=(10, 5))
    plt.plot(target_orig, label='Actual (Ground Truth)', linewidth=2)
    plt.plot(pred_orig, label='Predicted (Moving Average)', linestyle='--', color='red', linewidth=2)
    plt.title(f"Moving Average Baseline Prediction (Sample Index: {target_sample_idx})")
    plt.xlabel("Time Steps (Next 24 Hours)")
    plt.ylabel("Number of People")
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(f"{MODEL_SAVE_DIR}/prediction_result_baseline_ma.png")
    plt.show()
    print(f"Result plot saved to {MODEL_SAVE_DIR}/prediction_result_baseline_ma.png")

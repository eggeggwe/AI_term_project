{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8914a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.sans-serif\"] = [\n",
    "    \"Microsoft JhengHei\",\n",
    "    \"SimHei\",\n",
    "    \"Arial Unicode MS\",\n",
    "    \"DejaVu Sans\",\n",
    "]\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False  # è§£æ±ºè² è™Ÿé¡¯ç¤ºå•é¡Œ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903ce4c",
   "metadata": {},
   "source": [
    "1. åƒæ•¸è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70be654",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/task1_dataset_kotae.csv\"\n",
    "MODEL_SAVE_DIR = \"../models\"\n",
    "RESULT_SAVE_DIR = \"../models/hyperparameter_results\"\n",
    "\n",
    "# å›ºå®šåƒæ•¸\n",
    "INPUT_SEQ_LEN = 144  # è¼¸å…¥éå» 72 å°æ™‚\n",
    "OUTPUT_SEQ_LEN = 48  # é æ¸¬æœªä¾† 24 å°æ™‚\n",
    "BATCH_SIZE = 512\n",
    "INPUT_SIZE = 1\n",
    "OUTPUT_SIZE = 1\n",
    "\n",
    "# è³‡æ–™åˆ‡åˆ†è¨­å®š\n",
    "TRAIN_DAYS = 40\n",
    "VAL_DAYS = 10\n",
    "TEST_DAYS = 25\n",
    "\n",
    "# è¨“ç·´è¨­å®š\n",
    "EPOCHS = 300 \n",
    "PATIENCE = 20  # Early Stopping è€å¿ƒå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f01b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¸¬è©¦ä¸€ä¸‹æ˜¯å¦èƒ½æŠ“å–åˆ° CUDAï¼Œèª¿ç”¨ GPU é€²è¡Œè¨“ç·´\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "os.makedirs(RESULT_SAVE_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aaa670",
   "metadata": {},
   "source": [
    "2. è¶…åƒæ•¸å¯¦é©—é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89f0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPERPARAMETER_GRID = {\n",
    "    \"hidden_size\": [64, 128, 256],\n",
    "    \"num_layers\": [1, 2, 4],\n",
    "    \"learning_rate\": [0.001, 0.0005, 0.0001],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbf603e",
   "metadata": {},
   "source": [
    "3. è³‡æ–™è™•ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec36f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "    print(\"Loading raw data...\")\n",
    "    raw_df = pd.read_csv(path)\n",
    "    \n",
    "    # èšåˆè¨ˆç®—äººæ•¸\n",
    "    df = raw_df.groupby(['d', 't', 'x', 'y']).size().reset_index(name='number of people')\n",
    "    print(f\"Aggregated data shape: {df.shape}\")\n",
    "    \n",
    "    # ç¯©é¸å‰ä¸‰å¤§ç†±é»\n",
    "    top_3 = df.groupby(['x', 'y'])['number of people'].sum().nlargest(3).reset_index()[['x', 'y']]\n",
    "    result_df = pd.merge(df, top_3, on=['x', 'y'], how='inner')\n",
    "    \n",
    "    # æ¨™æº–åŒ–\n",
    "    scaler = MinMaxScaler()\n",
    "    result_df['number_scaled'] = scaler.fit_transform(result_df[['number of people']])\n",
    "    \n",
    "    return result_df, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39661112",
   "metadata": {},
   "source": [
    "4. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcddee69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, df, group_by_cols, target_col, input_seq_len, output_seq_len):\n",
    "        self.sequences = []\n",
    "        grouped = df.groupby(group_by_cols)\n",
    "\n",
    "        for _, group_df in grouped:\n",
    "            group_df = group_df.sort_values([\"d\", \"t\"])\n",
    "            values = group_df[target_col].values\n",
    "            total_len = len(values)\n",
    "\n",
    "            for i in range(total_len - input_seq_len - output_seq_len + 1):\n",
    "                input_seq = values[i : i + input_seq_len]\n",
    "                output_seq = values[\n",
    "                    i + input_seq_len : i + input_seq_len + output_seq_len\n",
    "                ]\n",
    "                self.sequences.append((input_seq, output_seq))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_seq, output_seq = self.sequences[idx]\n",
    "        input_tensor = torch.FloatTensor(input_seq).unsqueeze(-1)\n",
    "        output_tensor = torch.FloatTensor(output_seq).unsqueeze(-1)\n",
    "        return input_tensor, output_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b83a439",
   "metadata": {},
   "source": [
    "5. æ¨¡å‹æ¶æ§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e0e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, cell) = self.lstm(x)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.lstm(x, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, target_len, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.target_len = target_len\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src):\n",
    "        batch_size = src.shape[0]\n",
    "        output_size = self.decoder.output_size\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        decoder_input = torch.zeros(\n",
    "            batch_size, 1, output_size, device=src.device\n",
    "        ).float()\n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(self.target_len):\n",
    "            prediction, hidden, cell = self.decoder(decoder_input, hidden, cell)\n",
    "            outputs.append(prediction)\n",
    "            decoder_input = prediction.unsqueeze(1)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01c62f",
   "metadata": {},
   "source": [
    "6. è¨“ç·´å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70363920",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_loader, val_loader, optimizer, criterion, epochs, patience\n",
    "):\n",
    "    \"\"\"è¨“ç·´æ¨¡å‹ä¸¦å›å‚³æœ€ä½³é©—è­‰ Loss å’Œè¨“ç·´æ­·å²\"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # è¨“ç·´éšæ®µ\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # é©—è­‰éšæ®µ\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "                output = model(x)\n",
    "                val_loss = criterion(output, y)\n",
    "                total_val_loss += val_loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "        # Early Stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "\n",
    "    # è¼‰å…¥æœ€ä½³æ¨¡å‹\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return best_val_loss, history, epoch + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bd740f",
   "metadata": {},
   "source": [
    "7. è©•ä¼°å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca755a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler):\n",
    "    \"\"\"è©•ä¼°æ¨¡å‹ä¸¦å›å‚³ MSE, RMSE, MAE\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            output = model(x)\n",
    "            all_preds.append(output.cpu().numpy())\n",
    "            all_targets.append(y.numpy())\n",
    "\n",
    "    preds = np.concatenate(all_preds, axis=0).reshape(-1, 1)\n",
    "    targets = np.concatenate(all_targets, axis=0).reshape(-1, 1)\n",
    "\n",
    "    preds_original = scaler.inverse_transform(preds).flatten()\n",
    "    targets_original = scaler.inverse_transform(targets).flatten()\n",
    "\n",
    "    mse = mean_squared_error(targets_original, preds_original)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(targets_original, preds_original)\n",
    "\n",
    "    return mse, rmse, mae\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219574a9",
   "metadata": {},
   "source": [
    "8. ä¸»åŸ·è¡Œæµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e9eda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: æº–å‚™è³‡æ–™\n",
    "df, scaler = load_and_preprocess_data(DATA_PATH)\n",
    "\n",
    "train_df = df[df[\"d\"] < TRAIN_DAYS]\n",
    "val_df = df[(df[\"d\"] >= TRAIN_DAYS) & (df[\"d\"] < TRAIN_DAYS + VAL_DAYS)]\n",
    "test_df = df[df[\"d\"] >= TRAIN_DAYS + VAL_DAYS]\n",
    "\n",
    "train_dataset = GridTimeSeriesDataset(\n",
    "    train_df, [\"x\", \"y\"], \"number_scaled\", INPUT_SEQ_LEN, OUTPUT_SEQ_LEN\n",
    ")\n",
    "val_dataset = GridTimeSeriesDataset(\n",
    "    val_df, [\"x\", \"y\"], \"number_scaled\", INPUT_SEQ_LEN, OUTPUT_SEQ_LEN\n",
    ")\n",
    "test_dataset = GridTimeSeriesDataset(\n",
    "    test_df, [\"x\", \"y\"], \"number_scaled\", INPUT_SEQ_LEN, OUTPUT_SEQ_LEN\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\n",
    "    f\"è¨“ç·´æ¨£æœ¬: {len(train_dataset)}, é©—è­‰æ¨£æœ¬: {len(val_dataset)}, æ¸¬è©¦æ¨£æœ¬: {len(test_dataset)}\"\n",
    ")\n",
    "\n",
    "# Step 2: è¶…åƒæ•¸å¯¦é©—\n",
    "results = []\n",
    "experiment_id = 0\n",
    "total_experiments = (\n",
    "    len(HYPERPARAMETER_GRID[\"hidden_size\"])\n",
    "    * len(HYPERPARAMETER_GRID[\"num_layers\"])\n",
    "    * len(HYPERPARAMETER_GRID[\"learning_rate\"])\n",
    ")\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"é–‹å§‹è¶…åƒæ•¸å¯¦é©— (å…± {total_experiments} çµ„)\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "for hidden_size in HYPERPARAMETER_GRID[\"hidden_size\"]:\n",
    "    for num_layers in HYPERPARAMETER_GRID[\"num_layers\"]:\n",
    "        for learning_rate in HYPERPARAMETER_GRID[\"learning_rate\"]:\n",
    "            experiment_id += 1\n",
    "            print(\n",
    "                f\"\\n[{experiment_id}/{total_experiments}] Hidden={hidden_size}, Layers={num_layers}, LR={learning_rate}\"\n",
    "            )\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # å»ºç«‹æ¨¡å‹\n",
    "            encoder = Encoder(INPUT_SIZE, hidden_size, num_layers).to(DEVICE)\n",
    "            decoder = Decoder(OUTPUT_SIZE, hidden_size, num_layers).to(DEVICE)\n",
    "            model = Seq2Seq(encoder, decoder, OUTPUT_SEQ_LEN, DEVICE).to(DEVICE)\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            criterion = nn.MSELoss()\n",
    "\n",
    "            # è¨“ç·´\n",
    "            best_val_loss, history, stopped_epoch = train_model(\n",
    "                model,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                EPOCHS,\n",
    "                PATIENCE,\n",
    "            )\n",
    "\n",
    "            # è©•ä¼°\n",
    "            mse, rmse, mae = evaluate_model(model, test_loader, scaler)\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "\n",
    "            print(\n",
    "                f\"  â†’ MSE: {mse:.4f}, RMSE: {rmse:.4f}, MAE: {mae:.4f} (Epoch: {stopped_epoch}, Time: {elapsed_time:.1f}s)\"\n",
    "            )\n",
    "\n",
    "            # å„²å­˜çµæœ\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Hidden Size\": hidden_size,\n",
    "                    \"Num Layers\": num_layers,\n",
    "                    \"Learning Rate\": learning_rate,\n",
    "                    \"Best Val Loss\": best_val_loss,\n",
    "                    \"Test MSE\": mse,\n",
    "                    \"Test RMSE\": rmse,\n",
    "                    \"Test MAE\": mae,\n",
    "                    \"Stopped Epoch\": stopped_epoch,\n",
    "                    \"Train Time (s)\": elapsed_time,\n",
    "                }\n",
    "            )\n",
    "\n",
    "# Step 3: æ•´ç†çµæœè¡¨æ ¼\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(\"Test MSE\")\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"è¶…åƒæ•¸å¯¦é©—çµæœ (æŒ‰ Test MSE æ’åº)\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# å„²å­˜è¡¨æ ¼ç‚º CSV\n",
    "csv_path = f\"{RESULT_SAVE_DIR}/hyperparameter_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\nğŸ“ çµæœè¡¨æ ¼å·²å„²å­˜è‡³ {csv_path}\")\n",
    "\n",
    "# Step 4: è¦–è¦ºåŒ–\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 4.1 Hidden Size å° MSE çš„å½±éŸ¿\n",
    "ax1 = axes[0, 0]\n",
    "for lr in HYPERPARAMETER_GRID[\"learning_rate\"]:\n",
    "    subset = results_df[results_df[\"Learning Rate\"] == lr]\n",
    "    avg_by_hidden = subset.groupby(\"Hidden Size\")[\"Test MSE\"].mean()\n",
    "    ax1.plot(\n",
    "        avg_by_hidden.index, avg_by_hidden.values, marker=\"o\", label=f\"LR={lr}\"\n",
    "    )\n",
    "ax1.set_xlabel(\"Hidden Size\")\n",
    "ax1.set_ylabel(\"Test MSE\")\n",
    "ax1.set_title(\"Hidden Size vs Test MSE (by Learning Rate)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.2 Num Layers å° MSE çš„å½±éŸ¿\n",
    "ax2 = axes[0, 1]\n",
    "for lr in HYPERPARAMETER_GRID[\"learning_rate\"]:\n",
    "    subset = results_df[results_df[\"Learning Rate\"] == lr]\n",
    "    avg_by_layers = subset.groupby(\"Num Layers\")[\"Test MSE\"].mean()\n",
    "    ax2.plot(\n",
    "        avg_by_layers.index, avg_by_layers.values, marker=\"s\", label=f\"LR={lr}\"\n",
    "    )\n",
    "ax2.set_xlabel(\"Num Layers\")\n",
    "ax2.set_ylabel(\"Test MSE\")\n",
    "ax2.set_title(\"Num Layers vs Test MSE (by Learning Rate)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.3 Learning Rate å° MSE çš„å½±éŸ¿\n",
    "ax3 = axes[1, 0]\n",
    "for hs in HYPERPARAMETER_GRID[\"hidden_size\"]:\n",
    "    subset = results_df[results_df[\"Hidden Size\"] == hs]\n",
    "    avg_by_lr = subset.groupby(\"Learning Rate\")[\"Test MSE\"].mean()\n",
    "    ax3.plot(avg_by_lr.index, avg_by_lr.values, marker=\"^\", label=f\"Hidden={hs}\")\n",
    "ax3.set_xlabel(\"Learning Rate\")\n",
    "ax3.set_ylabel(\"Test MSE\")\n",
    "ax3.set_title(\"Learning Rate vs Test MSE (by Hidden Size)\")\n",
    "ax3.set_xscale(\"log\")\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4.4 Top 5 æœ€ä½³åƒæ•¸çµ„åˆ (Bar Chart)\n",
    "ax4 = axes[1, 1]\n",
    "top5 = results_df.head(5)\n",
    "labels = [\n",
    "    f\"H{r['Hidden Size']}-L{r['Num Layers']}-{r['Learning Rate']}\"\n",
    "    for _, r in top5.iterrows()\n",
    "]\n",
    "colors = [\"#2ecc71\", \"#3498db\", \"#9b59b6\", \"#f1c40f\", \"#e74c3c\"]\n",
    "bars = ax4.barh(labels[::-1], top5[\"Test MSE\"].values[::-1], color=colors[::-1])\n",
    "ax4.set_xlabel(\"Test MSE\")\n",
    "ax4.set_title(\"Top 5 è¶…åƒæ•¸çµ„åˆ (MSE è¶Šä½è¶Šå¥½)\")\n",
    "ax4.grid(True, alpha=0.3, axis=\"x\")\n",
    "\n",
    "# åœ¨ bar ä¸Šé¡¯ç¤ºæ•¸å€¼\n",
    "for bar, val in zip(bars, top5[\"Test MSE\"].values[::-1]):\n",
    "    ax4.text(\n",
    "        bar.get_width() + 1,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{val:.2f}\",\n",
    "        va=\"center\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Seq2Seq è¶…åƒæ•¸å¯¦é©—çµæœ\", fontsize=14, y=1.02)\n",
    "\n",
    "fig_path = f\"{RESULT_SAVE_DIR}/hyperparameter_comparison.png\"\n",
    "plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"ğŸ“Š è¦–è¦ºåŒ–åœ–è¡¨å·²å„²å­˜è‡³ {fig_path}\")\n",
    "\n",
    "# Step 5: è¼¸å‡ºæœ€ä½³åƒæ•¸\n",
    "best_result = results_df.iloc[0]\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"ğŸ† æœ€ä½³è¶…åƒæ•¸çµ„åˆ\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Hidden Size:   {best_result['Hidden Size']}\")\n",
    "print(f\"  Num Layers:    {best_result['Num Layers']}\")\n",
    "print(f\"  Learning Rate: {best_result['Learning Rate']}\")\n",
    "print(f\"  Test MSE:      {best_result['Test MSE']:.4f}\")\n",
    "print(f\"  Test RMSE:     {best_result['Test RMSE']:.4f}\")\n",
    "print(f\"  Test MAE:      {best_result['Test MAE']:.4f}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "\n",
    "# å„²å­˜æœ€ä½³åƒæ•¸åˆ°æ–‡å­—æª”\n",
    "summary_path = f\"{RESULT_SAVE_DIR}/best_hyperparameters.txt\"\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "    f.write(\"Seq2Seq è¶…åƒæ•¸å¯¦é©— - æœ€ä½³åƒæ•¸\\n\")\n",
    "    f.write(f\"å¯¦é©—æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "    f.write(f\"Hidden Size:   {best_result['Hidden Size']}\\n\")\n",
    "    f.write(f\"Num Layers:    {best_result['Num Layers']}\\n\")\n",
    "    f.write(f\"Learning Rate: {best_result['Learning Rate']}\\n\")\n",
    "    f.write(f\"\\n--- æ¸¬è©¦é›†è©•ä¼°çµæœ ---\\n\")\n",
    "    f.write(f\"MSE:  {best_result['Test MSE']:.4f}\\n\")\n",
    "    f.write(f\"RMSE: {best_result['Test RMSE']:.4f}\\n\")\n",
    "    f.write(f\"MAE:  {best_result['Test MAE']:.4f}\\n\")\n",
    "    f.write(\"=\" * 50 + \"\\n\")\n",
    "print(f\"ğŸ“ æœ€ä½³åƒæ•¸å·²å„²å­˜è‡³ {summary_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
